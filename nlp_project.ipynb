{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for NLP - Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RULES:\n",
    "\n",
    "* Do not create any additional cell\n",
    "\n",
    "* Fill in the blanks\n",
    "\n",
    "* All cells should be runnable (modulo trivial compatibility bugs that we'd fix)\n",
    "\n",
    "* 4 / 20 points will be allocated to the clarity of your code\n",
    "\n",
    "* Efficient code will have a bonus\n",
    "\n",
    "DELIVERABLE:\n",
    "\n",
    "* this notebook\n",
    "* the predictions of the SST test set\n",
    "\n",
    "DO NOT INCLUDE THE DATASETS IN THE DELIVERABLE.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DATA = r\"./data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Monolingual (English) word embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2vec():\n",
    "    def __init__(self, fname, nmax=100000):\n",
    "        self.load_wordvec(fname, nmax)\n",
    "        self.word2id = dict.fromkeys(self.word2vec.keys())\n",
    "        self.id2word = {v: k for k, v in self.word2id.items()}\n",
    "        self.embeddings = np.array(self.word2vec.values())\n",
    "    \n",
    "    def load_wordvec(self, fname, nmax):\n",
    "        self.word2vec = {}\n",
    "        with io.open(fname, encoding='utf-8') as f:\n",
    "            next(f)\n",
    "            for i, line in enumerate(f):\n",
    "                word, vec = line.split(' ', 1)\n",
    "                self.word2vec[word] = np.fromstring(vec, sep=' ')\n",
    "                if i == (nmax - 1):\n",
    "                    break\n",
    "        print('Loaded %s pretrained word vectors' % (len(self.word2vec)))\n",
    "        \n",
    "    def most_similar(self, w, K=5):\n",
    "        # K most similar words: self.score  -  np.argsort \n",
    "        scores_w = [self.score(w,w0) for w0 in self.word2vec] #all the scores between w and the other words in word2vec\n",
    "\n",
    "        if not ((min(scores_w)== 0) and (max(scores_w)==0)):\n",
    "            index_best_words = (np.argsort(scores_w)[len(scores_w)-K:len(scores_w)])\n",
    "            list_best_words_desc = [list(self.word2vec.keys())[i] for i in index_best_words]#in the reverse order\n",
    "            return(list_best_words_desc[::-1])\n",
    "        else:\n",
    "            print(str(w)+' not in the initial dictionnary!')\n",
    "            \n",
    "    def score(self, w1, w2):\n",
    "        # cosine similarity: np.dot  -  np.linalg.norm\n",
    "        if (w1 in self.word2vec)and(w2 in self.word2vec):\n",
    "            score_w1 = self.word2vec[w1]\n",
    "            score_w2 = self.word2vec[w2]\n",
    "            return np.dot(score_w1, score_w2.T)/(np.linalg.norm(score_w1)*np.linalg.norm(score_w2))\n",
    "        else:#if both w1 and w2 are not in the word2vec.keys()\n",
    "            return 0\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 55000 pretrained word vectors\n",
      "cat dog 0.671683666279249\n",
      "dog pet 0.6842064029669219\n",
      "dogs cats 0.7074389328052403\n",
      "paris france 0.7775108541288561\n",
      "germany berlin 0.7420295235998392\n",
      "['cat', 'cats', 'kitty', 'kitten', 'feline']\n",
      "['dog', 'dogs', 'puppy', 'Dog', 'doggie']\n",
      "['dogs', 'dog', 'Dogs', 'doggies', 'canines']\n",
      "['paris', 'france', 'Paris', 'london', 'berlin']\n",
      "['germany', 'europe', 'german', 'berlin', 'france']\n"
     ]
    }
   ],
   "source": [
    "w2v = Word2vec(os.path.join(PATH_TO_DATA, 'crawl-300d-200k.vec'), nmax=55000)\n",
    "\n",
    "# You will be evaluated on the output of the following:\n",
    "for w1, w2 in zip(('cat', 'dog', 'dogs', 'paris', 'germany'), ('dog', 'pet', 'cats', 'france', 'berlin')):\n",
    "    print(w1, w2, w2v.score(w1, w2))\n",
    "for w1 in ['cat', 'dog', 'dogs', 'paris', 'germany']:\n",
    "    print(w2v.most_similar(w1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoV():\n",
    "    def __init__(self, w2v):\n",
    "        self.w2v = w2v\n",
    "        self.idf = None\n",
    "    \n",
    "    def encode(self, sentences, idf=False):\n",
    "        # takes a list of sentences, outputs a numpy array of sentence embeddings\n",
    "        # see TP1 for help\n",
    "        sentemb = []\n",
    "        j = 0\n",
    "        for sent in sentences:\n",
    "            if idf is False:\n",
    "                # mean of word vectors\n",
    "                list_words = [w for w in sent if w in self.w2v.word2vec]\n",
    "                if list_words:\n",
    "                    embeddings = [self.w2v.word2vec[w] for w in list_words]\n",
    "                else:\n",
    "                    embeddings = np.full_like(np.arange(300, dtype=int), np.nan, dtype=np.double)\n",
    "                m = np.nanmean(embeddings, axis=0)\n",
    "            else:\n",
    "                # idf-weighted mean of word vectors\n",
    "                list_words = [w for w in sent if w in self.w2v.word2vec]\n",
    "                if list_words:\n",
    "                    embeddings = [self.idf[w] * self.w2v.word2vec[w] for w in list_words]\n",
    "                else:\n",
    "                    embeddings = np.full_like(np.arange(300, dtype=int), np.nan, dtype=np.double)\n",
    "                m = np.nanmean(embeddings, axis=0)\n",
    "            sentemb.append(m)\n",
    "        return np.vstack(sentemb)\n",
    "\n",
    "\n",
    "    def most_similar(self, s, sentences, idf=False, K=5):\n",
    "        # get most similar sentences and **print** them\n",
    "        \n",
    "#         keys = self.encode(sentences, idf)\n",
    "#         query = self.encode([s], idf)[0]\n",
    "        \n",
    "        scores = [self.score(sent, s, idf) for sent in sentences]\n",
    "        scores_index = np.argsort(scores)[::-1]\n",
    "        \n",
    "        # The most similar sentence is the one with second best score (self-similarity is always 1)\n",
    "        most_similar_sentences_ = [sentences[scores_index[i]] for i in range(1, K + 1)]\n",
    "            \n",
    "        S = ' '.join(s)\n",
    "        print(str(K)+' most similar sentences to '+ str(' '.join(s))+' :')\n",
    "        for sent in most_similar_sentences_:\n",
    "            print(str(' '.join(sent)))\n",
    "        \n",
    "        return most_similar_sentences_\n",
    "\n",
    "\n",
    "    def score(self, s1, s2, idf=False):\n",
    "        # cosine similarity: use   np.dot  and  np.linalg.norm\n",
    "        enc_s1 = self.encode([s1], idf).flatten()\n",
    "        enc_s2 = self.encode([s2], idf).flatten()\n",
    "        \n",
    "        cond = ((np.linalg.norm(enc_s1)!=0) and ((np.linalg.norm(enc_s2)!=0)))\n",
    "        score_ = np.dot(enc_s1, enc_s2.T)/(np.linalg.norm(enc_s1)*np.linalg.norm(enc_s2)) if cond else 0\n",
    "        \n",
    "        return score_\n",
    "    \n",
    "    def build_idf(self, sentences):\n",
    "        # build the idf dictionary: associate each word to its idf value\n",
    "        idf = {}\n",
    "        for sent in sentences:\n",
    "            for w in set(sent):\n",
    "                idf[w] = idf.get(w, 0) + 1 \n",
    "        \n",
    "        for word in idf:\n",
    "            idf[word]= max(1, np.log10(len(sentences) / (idf[word])))\n",
    "            \n",
    "        self.idf = idf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 50000 pretrained word vectors\n",
      "\n",
      "\n",
      "5 most similar sentences to 1 smiling african american boy . :\n",
      "an african american man smiling .\n",
      "a little african american boy and girl looking up .\n",
      "an african american in sunglasses and a white tee-shirt smiles .\n",
      "an afican american woman standing behind two small african american children .\n",
      "an african american man is sitting .\n",
      "The score between sentence 7 and 13 is 0.5726258859719607\n",
      "\n",
      "\n",
      "5 most similar sentences to 1 smiling african american boy . :\n",
      "an african american man smiling .\n",
      "an african american man is sitting .\n",
      "a little african american boy and girl looking up .\n",
      "an afican american woman standing behind two small african american children .\n",
      "an african american woman braiding another african american woman 's hair , while sitting in a white chair .\n",
      "The score between sentence 7 and 13 is 0.4751450875368781\n"
     ]
    }
   ],
   "source": [
    "w2v = Word2vec(os.path.join(PATH_TO_DATA, 'crawl-300d-200k.vec'), nmax=50000)\n",
    "s2v = BoV(w2v)\n",
    "\n",
    "# Load sentences in \"PATH_TO_DATA/sentences.txt\"\n",
    "with open(os.path.join(PATH_TO_DATA,'sentences.txt')) as f:\n",
    "    sentences = f.readlines()\n",
    "    \n",
    "sentences = [sent.split() for sent in sentences]\n",
    "\n",
    "# Build idf scores for each word\n",
    "s2v.build_idf(sentences)\n",
    "\n",
    "# You will be evaluated on the output of the following:\n",
    "# s2v.most_similar('' if not sentences else sentences[10], sentences)  # BoV-mean\n",
    "# s2v.score('' if not sentences else sentences[7], '' if not sentences else sentences[13])\n",
    "print('\\n')\n",
    "s2v.most_similar(sentences[10], sentences)  # BoV-mean\n",
    "print('The score between sentence 7 and 13 is ' +str(s2v.score(sentences[7],sentences[13])))\n",
    "print('\\n')\n",
    "s2v.most_similar(sentences[10], sentences, True)  # BoV-idf\n",
    "print('The score between sentence 7 and 13 is '+str(s2v.score(sentences[7],sentences[13], True)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Multilingual (English-French) word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider a bilingual dictionary of size V_a (e.g French-English).\n",
    "\n",
    "Let's define **X** and **Y** the **French** and **English** matrices.\n",
    "\n",
    "They contain the embeddings associated to the words in the bilingual dictionary.\n",
    "\n",
    "We want to find a **mapping W** that will project the source word space (e.g French) to the target word space (e.g English).\n",
    "\n",
    "Procrustes : **W\\* = argmin || W.X - Y ||  s.t  W^T.W = Id**\n",
    "has a closed form solution:\n",
    "**W = U.V^T  where  U.Sig.V^T = SVD(Y.X^T)**\n",
    "\n",
    "In what follows, you are asked to: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 50000 pretrained word vectors\n",
      "Loaded 50000 pretrained word vectors\n"
     ]
    }
   ],
   "source": [
    "# 1 - Download and load 50k first vectors of\n",
    "#     https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.en.vec\n",
    "#     https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.fr.vec\n",
    "\n",
    "fr_txt = Word2vec(os.path.join(PATH_TO_DATA, 'wiki.fr.vec'), nmax=50000)\n",
    "eng_txt = Word2vec(os.path.join(PATH_TO_DATA, 'wiki.en.vec'), nmax=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 - Get words that appear in both vocabs (= identical character strings)\n",
    "#     Use it to create the matrix X and Y (of aligned embeddings for these words)\n",
    "\n",
    "words_fr_txt = set(fr_txt.word2vec.keys())\n",
    "words_eng_txt = set(eng_txt.word2vec.keys())\n",
    "    \n",
    "identical_words = words_fr_txt & words_eng_txt #bilingual words\n",
    "    \n",
    "X = np.vstack([fr_txt.word2vec[w] for w in identical_words])\n",
    "Y = np.vstack([eng_txt.word2vec[w] for w in identical_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 - Solve the Procrustes using the scipy package and: scipy.linalg.svd() and get the optimal W\n",
    "#     Now W*French_vector is in the same space as English_vector\n",
    "\n",
    "import scipy.linalg\n",
    "    \n",
    "U, s, V = scipy.linalg.svd(np.dot(Y.T, X)) #Singular Value Decomposition\n",
    "    \n",
    "W = np.dot(U, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---from French to English---\n",
      "5 best translations of ordinateur are: computers computer mainframe workstation programmable\n",
      "5 best translations of heure are: minutes hours hour hrs mins\n",
      "5 best translations of aimer are: love nous regrets affection affections\n",
      "\n",
      "\n",
      "---from English to French---\n",
      "5 best translations of computer are: computer informatique ordinateurs ordinateur computing\n",
      "5 best translations of apple are: apple microsoft macintosh iphone intel\n",
      "5 best translations of president are: président présidence présidents president coprésident\n"
     ]
    }
   ],
   "source": [
    "class Translation():\n",
    "    def __init__(self, w2v_lang1, w2v_lang2, nmax=100000):\n",
    "\n",
    "        self.w2v_lang1 = w2v_lang1.word2vec\n",
    "        self.w2v_lang2 = w2v_lang2.word2vec\n",
    "\n",
    "        self.word2id_lang1 = dict.fromkeys(self.w2v_lang1.keys())\n",
    "        self.word2id_lang2 = dict.fromkeys(self.w2v_lang2.keys())\n",
    "        \n",
    "        self.id2word_lang1 = {v: k for k, v in self.word2id_lang1.items()}\n",
    "        self.id2word_lang2 = {v: k for k, v in self.word2id_lang2.items()}\n",
    "        \n",
    "        self.W = W\n",
    "        \n",
    "    def Translate(self, w, K=5, language='French'):\n",
    "\n",
    "        if language=='French':\n",
    "            W1 = self.W\n",
    "            id2word = self.id2word_lang2\n",
    "            w2v = self.w2v_lang1\n",
    "            w2v_foreign = self.w2v_lang2\n",
    "            \n",
    "        elif language=='English':\n",
    "            W1 = np.linalg.inv(self.W)\n",
    "            id2word = self.id2word_lang1\n",
    "            w2v = self.w2v_lang2\n",
    "            w2v_foreign = self.w2v_lang1\n",
    "        \n",
    "        scores = [self.score(np.dot(W1,w2v[w]), w2) for w2 in w2v_foreign.values()]\n",
    "        index_best_words = (np.argsort(scores)[len(scores)-K:len(scores)])\n",
    "        list_best_words_desc = [list(w2v_foreign.keys())[i] for i in index_best_words]#in the reverse order\n",
    "        return(list_best_words_desc[::-1])\n",
    "\n",
    "    def score(self, w1, w2):\n",
    "        if (np.linalg.norm(w1)*np.linalg.norm(w2))!=0:\n",
    "            return w1.dot(w2)/(np.linalg.norm(w1)*np.linalg.norm(w2))\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "\n",
    "trad = Translation(fr_txt, eng_txt)\n",
    "\n",
    "print('---from French to English---')\n",
    "fr_words = [\"ordinateur\", \"heure\", \"aimer\"]\n",
    "for w in fr_words:\n",
    "    t = trad.Translate(w, language='French')\n",
    "    print(\"5 best translations of {} are: {}\".format(w, \" \".join(t)))\n",
    "print('\\n')\n",
    "print('---from English to French---')\n",
    "en_words = [\"computer\", \"apple\", \"president\"]\n",
    "for w in en_words:\n",
    "    t = trad.Translate(w, language='English')\n",
    "    print(\"5 best translations of {} are: {}\".format(w, \" \".join(t)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to dive deeper on this subject: https://github.com/facebookresearch/MUSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Sentence classification with BoV and scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 - Load train/dev/test of Stanford Sentiment TreeBank (SST)\n",
    "#     (https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf)\n",
    "\n",
    "train_file = open(os.path.join(PATH_TO_DATA, 'SST/stsa.fine.train'))\n",
    "dev_file = open(os.path.join(PATH_TO_DATA, 'SST/stsa.fine.dev'))\n",
    "test_file = open(os.path.join(PATH_TO_DATA, 'SST/stsa.fine.test.X'))\n",
    "    \n",
    "def load_file(file, is_test = False):\n",
    "    A = []\n",
    "    categorical = []                \n",
    "    if not is_test:\n",
    "        for l in file:\n",
    "            A.append(l.split()[1:])\n",
    "            categorical.append(l.split()[0])\n",
    "        categorical = [int(categorical[i]) for i in range(len(categorical))]\n",
    "        return A, categorical\n",
    "    else:\n",
    "        for l in file:\n",
    "            A.append(l.split())\n",
    "        return A\n",
    "\n",
    "X_train, Y_train = load_file(train_file)\n",
    "X_dev, Y_dev = load_file(dev_file)\n",
    "X_test = load_file(test_file, is_test = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 55000 pretrained word vectors\n"
     ]
    }
   ],
   "source": [
    "# 2 - Encode sentences with the BoV model above\n",
    "\n",
    "w2v = Word2vec(os.path.join(PATH_TO_DATA, 'crawl-300d-200k.vec'), nmax=55000)\n",
    "s2v = BoV(w2v)\n",
    "\n",
    "s2v.build_idf(X_train+X_dev+X_test)\n",
    "\n",
    "#without idf\n",
    "Xtrain_encode = s2v.encode(X_train)\n",
    "Xdev_encode = s2v.encode(X_dev)\n",
    "Xtest_encode = s2v.encode(X_test)\n",
    "\n",
    "#with idf\n",
    "Xtrain_encode_idf = s2v.encode(X_train, idf=True)\n",
    "Xdev_encode_idf = s2v.encode(X_dev, idf=True)\n",
    "Xtest_encode_idf = s2v.encode(X_test, idf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------Without idf--------\n",
      "accuracy score for train is  0.46769662921348315\n",
      "accuracy score for dev is 0.4250681198910082\n",
      "-------With idf--------\n",
      "accuracy score for train is  0.4621956928838951\n",
      "accuracy score for dev is 0.4268846503178928\n"
     ]
    }
   ],
   "source": [
    "# 3 - Learn Logistic Regression on top of sentence embeddings using scikit-learn\n",
    "#     (consider tuning the L2 regularization on the dev set)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "acc_list_train = []\n",
    "acc_list_dev =[]\n",
    "for C in np.linspace(0.01,0.3, 20):\n",
    "    logistic_model = LogisticRegression(C=C,max_iter=1000)#By default it's penality l2\n",
    "    logistic_model.fit(Xtrain_encode,Y_train)\n",
    "\n",
    "    pred_train = logistic_model.predict( Xtrain_encode)\n",
    "    pred_dev = logistic_model.predict(Xdev_encode)\n",
    "    \n",
    "    acc_list_train.append(accuracy_score(Y_train,pred_train))\n",
    "    acc_list_dev.append(accuracy_score(Y_dev,pred_dev))\n",
    "    \n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(np.linspace(0.01,0.3, 20), acc_list_train, label = 'Train')\n",
    "plt.plot(np.linspace(0.01,0.3, 20), acc_list_dev, label = 'Test')\n",
    "plt.xlabel('C')\n",
    "plt.ylabel('accuracy score')\n",
    "plt.title('accuracy score with C')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "logistic_model = LogisticRegression(C=0.3,max_iter=1000)#By default it's penality l2\n",
    "logistic_model.fit(Xtrain_encode,Y_train)\n",
    "\n",
    "pred_train = logistic_model.predict( Xtrain_encode)\n",
    "pred_dev = logistic_model.predict(Xdev_encode)\n",
    "\n",
    "print('-------Without idf--------')\n",
    "print('accuracy score for train is ', accuracy_score(Y_train,pred_train))\n",
    "print('accuracy score for dev is', accuracy_score(Y_dev,pred_dev))\n",
    "\n",
    "\n",
    "logistic_model_idf = LogisticRegression(C=0.04,max_iter=1000)#By default it's penality l2\n",
    "logistic_model_idf.fit(Xtrain_encode_idf,Y_train)\n",
    "\n",
    "pred_train_idf = logistic_model_idf.predict(Xtrain_encode_idf)\n",
    "pred_dev_idf = logistic_model_idf.predict(Xdev_encode_idf)\n",
    "\n",
    "print('-------With idf--------')\n",
    "print('accuracy score for train is ', accuracy_score(Y_train,pred_train_idf))\n",
    "print('accuracy score for dev is', accuracy_score(Y_dev,pred_dev_idf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 - Produce 2210 predictions for the test set (in the same order). One line = one prediction (=0,1,2,3,4).\n",
    "#     Attach the output file \"logreg_bov_y_test_sst.txt\" to your deliverable.\n",
    "#     You will be evaluated on the results of the test set.\n",
    "\n",
    "\n",
    "lines = '\\n'.join([str(c) for c in logistic_model.predict(Xtest_encode)])\n",
    "with open(os.path.join('.',r'logreg_bov_y_test_sst.txt'),'w') as f:\n",
    "    f.writelines(lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score with XGBoost is 0.40054495912806537\n",
      "accuracy score with LDA is 0.4332425068119891\n"
     ]
    }
   ],
   "source": [
    "# BONUS!\n",
    "# 5 - Try to improve performance with another classifier\n",
    "#     Attach the output file \"XXX_bov_y_test_sst.txt\" to your deliverable (where XXX = the name of the classifier)\n",
    "from xgboost import XGBClassifier\n",
    "    \n",
    "xgb = XGBClassifier() \n",
    "xgb.fit(Xtrain_encode, Y_train,eval_set=[(Xdev_encode, Y_dev)], early_stopping_rounds=20, verbose = False)\n",
    "pred = xgb.predict(Xdev_encode)\n",
    "print('accuracy score with XGBoost is '+str(accuracy_score(Y_dev, pred)))\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "lda = LDA(n_components=3)\n",
    "X_lda = lda.fit(Xtrain_encode, Y_train)\n",
    "pred = X_lda.predict(Xdev_encode)\n",
    "print('accuracy score with LDA is '+str(accuracy_score(Y_dev, pred)))\n",
    "\n",
    "lines = '\\n'.join([str(c) for c in X_lda.predict(Xtest_encode)])\n",
    "with open(os.path.join('.',r'lda_bov_y_test_sst.txt'),'w') as f:\n",
    "    f.writelines(lines)\n",
    "    \n",
    "#Linear model perform better than non linear models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Sentence classification with LSTMs in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 - Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 - Load train/dev/test sets of SST\n",
    "PATH_TO_DATA = \"./data/\"\n",
    "train_file = open(os.path.join(PATH_TO_DATA, 'SST/stsa.fine.train'))\n",
    "dev_file = open(os.path.join(PATH_TO_DATA, 'SST/stsa.fine.dev'))\n",
    "test_file = open(os.path.join(PATH_TO_DATA, 'SST/stsa.fine.test.X'))\n",
    "\n",
    "X_train, Y_train = load_file(train_file)\n",
    "X_dev, Y_dev = load_file(dev_file)\n",
    "X_test = load_file(test_file, is_test = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 - Transform text to integers using keras.preprocessing.text.one_hot function\n",
    "#     https://keras.io/preprocessing/text/\n",
    "\n",
    "from keras.preprocessing import text\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "w1 = set([s for sent in (X_train+X_dev+X_test) for s in sent ])\n",
    "vocab_size = len(w1)\n",
    "\n",
    "X_train_one_hot = [text.one_hot(' '.join(x),vocab_size) for x in X_train]\n",
    "X_dev_one_hot = [text.one_hot(' '.join(x),vocab_size) for x in X_dev]\n",
    "X_test_one_hot = [text.one_hot(' '.join(x),vocab_size) for x in X_test]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Padding input data**\n",
    "\n",
    "Models in Keras (and elsewhere) take batches of sentences of the same length as input. It is because Deep Learning framework have been designed to handle well Tensors, which are particularly suited for fast computation on the GPU.\n",
    "\n",
    "Since sentences have different sizes, we \"pad\" them. That is, we add dummy \"padding\" tokens so that they all have the same length.\n",
    "\n",
    "The input to a Keras model thus has this size : (batchsize, maxseqlen) where maxseqlen is the maximum length of a sentence in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 - Pad your sequences using keras.preprocessing.sequence.pad_sequences\n",
    "#     https://keras.io/preprocessing/sequence/\n",
    "\n",
    "# TYPE CODE HERE\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "X_train_pad = pad_sequences(X_train_one_hot, maxlen= 40)\n",
    "X_dev_pad = pad_sequences(X_dev_one_hot, maxlen= 40)\n",
    "X_test_pad = pad_sequences(X_test_one_hot, maxlen= 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 - Design and train your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 - Design your encoder + classifier using keras.layers\n",
    "#     In Keras, Torch and other deep learning framework, we create a \"container\" which is the Sequential() module.\n",
    "#     Then we add components to this contained : the lookuptable, the LSTM, the classifier etc.\n",
    "#     All of these components are contained in the Sequential() and are trained together.\n",
    "\n",
    "\n",
    "# ADAPT CODE BELOW\n",
    "\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Activation, SpatialDropout1D\n",
    "\n",
    "embed_dim  = 32  # word embedding dimension\n",
    "nhid       = 64  # number of hidden units in the LSTM\n",
    "vocab_size = vocab_size  # size of the vocabulary\n",
    "n_classes  = 5\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embed_dim))\n",
    "model.add(SpatialDropout1D(0.7))\n",
    "model.add(LSTM(nhid, dropout=0.7, recurrent_dropout=0.7))\n",
    "model.add(Dense(n_classes, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 32)          625216    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, None, 32)          0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 64)                24832     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 650,373\n",
      "Trainable params: 650,373\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# 5 - Define your loss/optimizer/metrics\n",
    "\n",
    "# MODIFY CODE BELOW\n",
    "loss_classif     =  'categorical_crossentropy' # find the right loss for multi-class classification\n",
    "optimizer        =  'nadam' # find the right optimizer\n",
    "metrics_classif  =  ['accuracy']\n",
    "\n",
    "# Observe how easy (but blackboxed) this is in Keras\n",
    "model.compile(loss=loss_classif,\n",
    "              optimizer=optimizer,\n",
    "              metrics=metrics_classif)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8544 samples, validate on 1101 samples\n",
      "Epoch 1/6\n",
      "8544/8544 [==============================] - 11s 1ms/step - loss: 1.5773 - acc: 0.2621 - val_loss: 1.5708 - val_acc: 0.2534\n",
      "Epoch 2/6\n",
      "8544/8544 [==============================] - 9s 1ms/step - loss: 1.5646 - acc: 0.2864 - val_loss: 1.5620 - val_acc: 0.3079\n",
      "Epoch 3/6\n",
      "8544/8544 [==============================] - 10s 1ms/step - loss: 1.5361 - acc: 0.3143 - val_loss: 1.5166 - val_acc: 0.3433\n",
      "Epoch 4/6\n",
      "8544/8544 [==============================] - 10s 1ms/step - loss: 1.4882 - acc: 0.3490 - val_loss: 1.4814 - val_acc: 0.3569\n",
      "Epoch 5/6\n",
      "8544/8544 [==============================] - 10s 1ms/step - loss: 1.4454 - acc: 0.3711 - val_loss: 1.4581 - val_acc: 0.3542\n",
      "Epoch 6/6\n",
      "8544/8544 [==============================] - 10s 1ms/step - loss: 1.3956 - acc: 0.3960 - val_loss: 1.4465 - val_acc: 0.3515\n",
      "---Plot---\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 6 - Train your model and find the best hyperparameters for your dev set\n",
    "#     you will be evaluated on the quality of your predictions on the test set\n",
    "\n",
    "# ADAPT CODE BELOW\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "bs = 64\n",
    "n_epochs = 6\n",
    "\n",
    "Y_train_categorical = to_categorical(Y_train)\n",
    "Y_dev_categorical = to_categorical(Y_dev)\n",
    "history = model.fit(X_train_pad, Y_train_categorical, batch_size=bs, nb_epoch=n_epochs, validation_data=(X_dev_pad, Y_dev_categorical), verbose =True)\n",
    "print('---Plot---')\n",
    "fig, (ax1, ax2) = plt.subplots(1,2)\n",
    "\n",
    "ax1.plot(history.history[\"loss\"], label=\"Train loss\")\n",
    "ax1.plot(history.history[\"val_loss\"], label=\"Val loss\")\n",
    "ax1.set_title(\"Loss\")\n",
    "ax1.set_xlabel(\"Epochs\")\n",
    "ax1.set_ylabel(\"Loss\")\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(history.history[\"acc\"], label=\"Train accuracy\")\n",
    "ax2.plot(history.history[\"val_acc\"], label=\"Val accuracy\")\n",
    "ax2.set_title(\"Accuracy\")\n",
    "ax2.set_xlabel(\"Epochs\")\n",
    "ax2.set_ylabel(\"Accuracy\")\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7 - Generate your predictions on the test set using model.predict(x_test)\n",
    "#     https://keras.io/models/model/\n",
    "#     Log your predictions in a file (one line = one integer: 0,1,2,3,4)\n",
    "#     Attach the output file \"logreg_lstm_y_test_sst.txt\" to your deliverable.\n",
    "\n",
    "# TYPE CODE HERE\n",
    "pred_test_one = model.predict(X_test_pad)\n",
    "pred_test = [np.argmax(pred) for pred in pred_test_one]\n",
    "lines = '\\n'.join([str(c) for c in pred_test])\n",
    "with open(os.path.join('.',r'logreg_lstm_y_test_sst.txt'),'w') as f:\n",
    "    f.writelines(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 -- innovate !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---filtering---\n",
      "Check 1: size of intersect of words in Train, Test and Dev is 17839\n",
      "---Creating Embedding Matrix---\n",
      "Check 2: Embedding Matrix size (17839, 300)\n",
      "---Train----\n",
      "Train on 8544 samples, validate on 1101 samples\n",
      "Epoch 1/6\n",
      "8544/8544 [==============================] - 25s 3ms/step - loss: 1.5299 - acc: 0.3082 - val_loss: 1.3993 - val_acc: 0.3806\n",
      "Epoch 2/6\n",
      "8544/8544 [==============================] - 21s 2ms/step - loss: 1.3836 - acc: 0.3910 - val_loss: 1.3274 - val_acc: 0.4114\n",
      "Epoch 3/6\n",
      "8544/8544 [==============================] - 21s 2ms/step - loss: 1.3432 - acc: 0.4129 - val_loss: 1.3131 - val_acc: 0.4205\n",
      "Epoch 4/6\n",
      "8544/8544 [==============================] - 20s 2ms/step - loss: 1.3137 - acc: 0.4260 - val_loss: 1.2746 - val_acc: 0.4151\n",
      "Epoch 5/6\n",
      "8544/8544 [==============================] - 20s 2ms/step - loss: 1.2909 - acc: 0.4397 - val_loss: 1.2671 - val_acc: 0.4233\n",
      "Epoch 6/6\n",
      "8544/8544 [==============================] - 22s 3ms/step - loss: 1.2736 - acc: 0.4496 - val_loss: 1.2666 - val_acc: 0.4387\n",
      "---Predict---\n",
      "---Saving The File---\n",
      "---End---\n"
     ]
    }
   ],
   "source": [
    "# 8 - Open question: find a model that is better on your dev set\n",
    "#     (e.g: use a 1D ConvNet, use a better classifier, pretrain your lookup tables ..)\n",
    "#     you will get point if the results on the test set are better: be careful of not overfitting your dev set too much..\n",
    "#     Attach the output file \"XXX_XXX_y_test_sst.txt\" to your deliverable.\n",
    "\n",
    "# TYPE CODE HERE\n",
    "from keras.layers import Bidirectional\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "train_file = open(os.path.join(PATH_TO_DATA, 'SST/stsa.fine.train'))\n",
    "dev_file = open(os.path.join(PATH_TO_DATA, 'SST/stsa.fine.dev'))\n",
    "test_file = open(os.path.join(PATH_TO_DATA, 'SST/stsa.fine.test.X'))\n",
    "\n",
    "\n",
    "def load_wordvec(fname, nmax):\n",
    "    word2vec = {}\n",
    "    with io.open(fname, encoding='utf-8') as f:\n",
    "        next(f)\n",
    "        for i, line in enumerate(f):\n",
    "            word, vec = line.split(' ', 1)\n",
    "            word2vec[word] = np.fromstring(vec, sep=' ')\n",
    "            if i == (nmax - 1):\n",
    "                break\n",
    "    return word2vec\n",
    "\n",
    "w2v = load_wordvec(os.path.join(PATH_TO_DATA, 'crawl-300d-200k.vec'), nmax=200000)\n",
    "\n",
    "def load_file(file, is_test = False):\n",
    "    A = []\n",
    "    categorical = []                \n",
    "    if not is_test:\n",
    "        for l in file:\n",
    "            A.append(l[1:])\n",
    "            categorical.append(l.split()[0])\n",
    "        categorical = [int(categorical[i]) for i in range(len(categorical))]\n",
    "        return A, categorical\n",
    "    else:\n",
    "        for l in file:\n",
    "            A.append(l)\n",
    "        return A\n",
    "\n",
    "X_train, Y_train = load_file(train_file)\n",
    "X_dev, Y_dev = load_file(dev_file)\n",
    "X_test = load_file(test_file, is_test = True)\n",
    "    \n",
    "#Pretrain of lookup tables and filtering the data\n",
    "print('---filtering---')\n",
    "X_train_token = [text.text_to_word_sequence(x) for x in X_train]\n",
    "X_dev_token = [text.text_to_word_sequence(x) for x in X_dev]\n",
    "X_test_token = [text.text_to_word_sequence(x) for x in X_test]\n",
    "\n",
    "interec = set()\n",
    "for x in X_train_token:\n",
    "    for y in x:\n",
    "        interec.add(y)\n",
    "for x in X_dev_token:\n",
    "    for y in x:\n",
    "        interec.add(y)\n",
    "for x in X_test_token:\n",
    "    for y in x:\n",
    "        interec.add(y)        \n",
    "\n",
    "print('Check 1: size of intersect of words in Train, Test and Dev is '+str(len(list(interec))))\n",
    "\n",
    "#For each word map it to a number in a dictionnary\n",
    "dict_={}\n",
    "count=0\n",
    "for x in list(interec):\n",
    "    dict_[count] = str(x)\n",
    "    count += 1\n",
    "            \n",
    "\n",
    "#Mapping the words of the dataFrames to the id of the dictionnary\n",
    "dict_inv = {v: k for k, v in dict_.items()}\n",
    "def Mapping(sent):\n",
    "    X = []\n",
    "    for s in sent:\n",
    "        mapp = []\n",
    "        for x in s:\n",
    "            mapp.append(dict_inv.get(x))\n",
    "        X.append(mapp)\n",
    "    return X\n",
    "\n",
    "X_train_pad = pad_sequences(Mapping(X_train_token), maxlen = 60)\n",
    "X_dev_pad = pad_sequences(Mapping(X_dev_token), maxlen = 60)\n",
    "X_test_pad = pad_sequences(Mapping(X_test_token), maxlen= 60)\n",
    "\n",
    "print('---Creating Embedding Matrix---')\n",
    "#Mapping between the id of dictionnary and the embedding\n",
    "Embedding_Matrix=[]\n",
    "for i in range(count):\n",
    "    Embedding_Matrix.append(w2v.get(dict_[i], np.zeros(300)))\n",
    "Embedding_Matrix = np.array(Embedding_Matrix)\n",
    "print('Check 2: Embedding Matrix size', Embedding_Matrix.shape)\n",
    "\n",
    "print('---Train----')\n",
    "model = Sequential()\n",
    "model.add(Embedding(Embedding_Matrix.shape[0], 300, trainable = False, weights = [Embedding_Matrix]))\n",
    "model.add(Bidirectional(LSTM(32, dropout_W=0.4, recurrent_dropout=0.4))) #bidirectional layer\n",
    "model.add(Dense(n_classes, activation='softmax'))\n",
    "\n",
    "loss_classif     =  'categorical_crossentropy' # find the right loss for multi-class classification\n",
    "optimizer        =  'adam' # find the right optimizer\n",
    "metrics_classif  =  ['accuracy']\n",
    "\n",
    "model.compile(loss=loss_classif,\n",
    "              optimizer=optimizer,\n",
    "              metrics=metrics_classif)\n",
    "\n",
    "\n",
    "bs = 64\n",
    "n_epochs = 6\n",
    "\n",
    "history = model.fit(X_train_pad, to_categorical(Y_train), \n",
    "                    batch_size=bs,\n",
    "                    epochs=n_epochs, \n",
    "                    validation_data=(X_dev_pad, to_categorical(Y_dev)), verbose=True)\n",
    "\n",
    "print('---Predict---')\n",
    "pred_test_one = model.predict(X_test_pad)\n",
    "pred_test = [np.argmax(pred) for pred in pred_test_one]\n",
    "print('---Saving The File---')\n",
    "lines = '\\n'.join([str(c) for c in pred_test])\n",
    "with open(os.path.join('.',r'bilstm_dense_y_test_sst.txt'),'w') as f:\n",
    "    f.writelines(lines)\n",
    "print('---End---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Aucun(e)",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
